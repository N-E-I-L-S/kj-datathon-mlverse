{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "document = \"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"p208p2002/bart-squad-qg-hl\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"p208p2002/bart-squad-qg-hl\")\n",
    "actual_answers = []\n",
    "\n",
    "def generate_random_questions_batch(documents, num_questions):\n",
    "    questions = []\n",
    "\n",
    "    for document in documents:\n",
    "        if len(document.split()) <= 1:\n",
    "            continue  # Skip empty or very short documents\n",
    "\n",
    "        text_lengths = [random.randint(1, min(len(document.split()) - 1, 10)) for _ in range(2)]\n",
    "\n",
    "        start_indices = [random.randint(0, len(document.split()) - text_length - 1) for text_length in text_lengths]\n",
    "\n",
    "        input_texts = [\n",
    "            \" \".join([\n",
    "                document.split()[i] if i < start_index or i >= start_index + text_length else \"[HL]\" \n",
    "                for i in range(len(document.split()))\n",
    "            ]) for start_index, text_length in zip(start_indices, text_lengths)\n",
    "        ]\n",
    "\n",
    "        temp_answers = []\n",
    "\n",
    "        for input_text in input_texts:\n",
    "            temp = input_text.split()\n",
    "            while \"[HL]\" in temp:\n",
    "                temp.remove(\"[HL]\")\n",
    "            string = \" \".join(temp)\n",
    "            temp_answers.append(string)\n",
    "\n",
    "        actual_answers.extend(temp_answers)\n",
    "\n",
    "        input_ids_list = [tokenizer(input_text, return_tensors=\"pt\").input_ids for input_text in input_texts]\n",
    "\n",
    "        # Assuming model input limit is 512 tokens, you may need to adjust this based on your model\n",
    "        max_tokens = 512\n",
    "\n",
    "        for input_ids in input_ids_list:\n",
    "            # Split input_ids into chunks of size max_tokens\n",
    "            for i in range(0, len(input_ids[0]), max_tokens):\n",
    "                input_ids_chunk = input_ids[:, i:i + max_tokens]\n",
    "\n",
    "                # Generate output for each chunk\n",
    "                output = model.generate(input_ids_chunk)\n",
    "\n",
    "                # Decode the generated output\n",
    "                generated_question = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                questions.append(generated_question)\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Split the document into smaller chunks for batch processing\n",
    "chunk_size = 300  # You may need to adjust this based on your document size\n",
    "document_chunks = [document[i:i + chunk_size] for i in range(0, len(document), chunk_size)]\n",
    "\n",
    "num_broader_questions = 3\n",
    "num_niche_questions = 2\n",
    "\n",
    "broader_questions = generate_random_questions_batch(document_chunks, num_broader_questions)\n",
    "broader_final_questions = random.sample(broader_questions, k=5)\n",
    "niche_questions = generate_random_questions_batch(document_chunks, num_niche_questions)\n",
    "niche_final_questions = random.sample(niche_questions, k=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
